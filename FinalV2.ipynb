{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pyGMM"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aJjP68sZ2K1i",
        "outputId": "61d61308-1077-4032-bd92-b1b046ac1d12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyGMM\n",
            "  Downloading pygmm-0.6.5-py2.py3-none-any.whl (954 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/954.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m174.1/954.5 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━\u001b[0m \u001b[32m614.4/954.5 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m952.3/954.5 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m954.5/954.5 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from pyGMM) (3.7.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pyGMM) (1.23.5)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.10/dist-packages (from pyGMM) (1.11.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from pyGMM) (1.16.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->pyGMM) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->pyGMM) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->pyGMM) (4.44.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->pyGMM) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->pyGMM) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->pyGMM) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->pyGMM) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->pyGMM) (2.8.2)\n",
            "Installing collected packages: pyGMM\n",
            "Successfully installed pyGMM-0.6.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RZ0JvNxNv2Md"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.optimizers import Adam"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data\n",
        "data = pd.read_excel('Cyprus_nM=50,dtp=15.xlsx')"
      ],
      "metadata": {
        "id": "gYcyPqkzwHCc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Shuffle Data\n",
        "shuffled_data = data.sample(frac=1, random_state=42)\n",
        "\n",
        "# Reset the index to avoid any issues with the shuffled data\n",
        "shuffled_data.reset_index(drop=True, inplace=True)\n",
        "\n",
        "data = shuffled_data"
      ],
      "metadata": {
        "id": "QoXnOf6xwKBc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Change response to categorical\n",
        "column_to_bin = 'Actual Mag'\n",
        "bin_edges = [0, 3.0, 5.0, 6.0, 10.0]\n",
        "\n",
        "bin_labels = ['Small', 'Medium', 'Large', 'Catastrophic']\n",
        "\n",
        "data['Binned_Column'] = pd.cut(data[column_to_bin], bins=bin_edges, labels=bin_labels)"
      ],
      "metadata": {
        "id": "RCGvZgHbwM1r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train Model\n",
        "net = MLPClassifier(hidden_layer_sizes=(20,), activation='logistic', max_iter=400)"
      ],
      "metadata": {
        "id": "1j4B_aa6wVf1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split your data into features (X) and labels (y)\n",
        "X = data.iloc[:, :60].values\n",
        "y = data.iloc[:, 60].values\n",
        "\n",
        "# Split the data into training, validation, and test sets\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=63)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.6, random_state=63)\n",
        "\n",
        "# Standardize the input data\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val = scaler.transform(X_val)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Create a neural network model\n",
        "model = Sequential()\n",
        "model.add(Dense(64, input_dim=60, activation='relu'))\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.001), metrics=['MSE'])\n",
        "\n",
        "# Train the model\n",
        "run = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=200, batch_size=32)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HFWfOy3Rwel3",
        "outputId": "2360aa88-6984-440a-eb6f-d7b552b1260b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "138/138 [==============================] - 3s 7ms/step - loss: -100.1207 - MSE: 6.7579 - val_loss: -412.9475 - val_MSE: 6.6258\n",
            "Epoch 2/200\n",
            "138/138 [==============================] - 1s 4ms/step - loss: -1895.6426 - MSE: 6.5129 - val_loss: -4160.7690 - val_MSE: 6.6258\n",
            "Epoch 3/200\n",
            "138/138 [==============================] - 0s 2ms/step - loss: -9152.4131 - MSE: 6.5129 - val_loss: -15059.6172 - val_MSE: 6.6258\n",
            "Epoch 4/200\n",
            "138/138 [==============================] - 0s 3ms/step - loss: -25563.3906 - MSE: 6.5129 - val_loss: -36372.5312 - val_MSE: 6.6258\n",
            "Epoch 5/200\n",
            "138/138 [==============================] - 0s 2ms/step - loss: -54032.8242 - MSE: 6.5129 - val_loss: -70659.2188 - val_MSE: 6.6258\n",
            "Epoch 6/200\n",
            "138/138 [==============================] - 1s 6ms/step - loss: -97654.9062 - MSE: 6.5129 - val_loss: -120979.5781 - val_MSE: 6.6258\n",
            "Epoch 7/200\n",
            "138/138 [==============================] - 1s 8ms/step - loss: -158917.7969 - MSE: 6.5129 - val_loss: -189061.2344 - val_MSE: 6.6258\n",
            "Epoch 8/200\n",
            "138/138 [==============================] - 1s 11ms/step - loss: -239763.0781 - MSE: 6.5129 - val_loss: -277254.8438 - val_MSE: 6.6258\n",
            "Epoch 9/200\n",
            "138/138 [==============================] - 1s 9ms/step - loss: -341752.4062 - MSE: 6.5129 - val_loss: -386204.6875 - val_MSE: 6.6258\n",
            "Epoch 10/200\n",
            "138/138 [==============================] - 1s 5ms/step - loss: -466156.9062 - MSE: 6.5129 - val_loss: -517687.3750 - val_MSE: 6.6258\n",
            "Epoch 11/200\n",
            "138/138 [==============================] - 0s 3ms/step - loss: -614744.0625 - MSE: 6.5129 - val_loss: -672669.8125 - val_MSE: 6.6258\n",
            "Epoch 12/200\n",
            "138/138 [==============================] - 0s 2ms/step - loss: -788413.5625 - MSE: 6.5129 - val_loss: -851763.5000 - val_MSE: 6.6258\n",
            "Epoch 13/200\n",
            "138/138 [==============================] - 0s 2ms/step - loss: -988396.0625 - MSE: 6.5129 - val_loss: -1058299.7500 - val_MSE: 6.6258\n",
            "Epoch 14/200\n",
            "138/138 [==============================] - 0s 2ms/step - loss: -1215553.8750 - MSE: 6.5129 - val_loss: -1290980.2500 - val_MSE: 6.6258\n",
            "Epoch 15/200\n",
            "138/138 [==============================] - 0s 2ms/step - loss: -1470768.3750 - MSE: 6.5129 - val_loss: -1550314.2500 - val_MSE: 6.6258\n",
            "Epoch 16/200\n",
            "138/138 [==============================] - 0s 2ms/step - loss: -1754842.8750 - MSE: 6.5129 - val_loss: -1836853.8750 - val_MSE: 6.6258\n",
            "Epoch 17/200\n",
            "138/138 [==============================] - 0s 2ms/step - loss: -2068253.0000 - MSE: 6.5129 - val_loss: -2153575.0000 - val_MSE: 6.6258\n",
            "Epoch 18/200\n",
            "138/138 [==============================] - 0s 2ms/step - loss: -2412584.2500 - MSE: 6.5129 - val_loss: -2499985.2500 - val_MSE: 6.6258\n",
            "Epoch 19/200\n",
            "138/138 [==============================] - 0s 2ms/step - loss: -2786828.0000 - MSE: 6.5129 - val_loss: -2874982.2500 - val_MSE: 6.6258\n",
            "Epoch 20/200\n",
            "138/138 [==============================] - 0s 2ms/step - loss: -3191622.5000 - MSE: 6.5129 - val_loss: -3280006.0000 - val_MSE: 6.6258\n",
            "Epoch 21/200\n",
            "138/138 [==============================] - 0s 2ms/step - loss: -3628464.7500 - MSE: 6.5129 - val_loss: -3713463.0000 - val_MSE: 6.6258\n",
            "Epoch 22/200\n",
            "138/138 [==============================] - 1s 4ms/step - loss: -4096910.2500 - MSE: 6.5129 - val_loss: -4183144.2500 - val_MSE: 6.6258\n",
            "Epoch 23/200\n",
            "138/138 [==============================] - 1s 5ms/step - loss: -4598115.5000 - MSE: 6.5129 - val_loss: -4682490.0000 - val_MSE: 6.6258\n",
            "Epoch 24/200\n",
            "138/138 [==============================] - 1s 5ms/step - loss: -5134495.5000 - MSE: 6.5129 - val_loss: -5213485.0000 - val_MSE: 6.6258\n",
            "Epoch 25/200\n",
            "138/138 [==============================] - 1s 5ms/step - loss: -5702587.0000 - MSE: 6.5129 - val_loss: -5774616.0000 - val_MSE: 6.6258\n",
            "Epoch 26/200\n",
            "138/138 [==============================] - 1s 4ms/step - loss: -6301147.5000 - MSE: 6.5129 - val_loss: -6367273.0000 - val_MSE: 6.6258\n",
            "Epoch 27/200\n",
            "138/138 [==============================] - 1s 4ms/step - loss: -6934480.5000 - MSE: 6.5129 - val_loss: -6995048.0000 - val_MSE: 6.6258\n",
            "Epoch 28/200\n",
            "138/138 [==============================] - 0s 2ms/step - loss: -7603130.0000 - MSE: 6.5129 - val_loss: -7655463.5000 - val_MSE: 6.6258\n",
            "Epoch 29/200\n",
            "138/138 [==============================] - 0s 2ms/step - loss: -8308957.0000 - MSE: 6.5129 - val_loss: -8353482.0000 - val_MSE: 6.6258\n",
            "Epoch 30/200\n",
            "138/138 [==============================] - 0s 2ms/step - loss: -9048645.0000 - MSE: 6.5129 - val_loss: -9083210.0000 - val_MSE: 6.6258\n",
            "Epoch 31/200\n",
            "138/138 [==============================] - 0s 2ms/step - loss: -9824786.0000 - MSE: 6.5129 - val_loss: -9848801.0000 - val_MSE: 6.6258\n",
            "Epoch 32/200\n",
            "138/138 [==============================] - 0s 2ms/step - loss: -10637654.0000 - MSE: 6.5129 - val_loss: -10650886.0000 - val_MSE: 6.6258\n",
            "Epoch 33/200\n",
            "138/138 [==============================] - 0s 2ms/step - loss: -11490007.0000 - MSE: 6.5129 - val_loss: -11487920.0000 - val_MSE: 6.6258\n",
            "Epoch 34/200\n",
            "138/138 [==============================] - 0s 2ms/step - loss: -12377593.0000 - MSE: 6.5129 - val_loss: -12357856.0000 - val_MSE: 6.6258\n",
            "Epoch 35/200\n",
            "138/138 [==============================] - 0s 2ms/step - loss: -13300804.0000 - MSE: 6.5129 - val_loss: -13265320.0000 - val_MSE: 6.6258\n",
            "Epoch 36/200\n",
            "138/138 [==============================] - 0s 4ms/step - loss: -14262228.0000 - MSE: 6.5129 - val_loss: -14212008.0000 - val_MSE: 6.6258\n",
            "Epoch 37/200\n",
            "138/138 [==============================] - 0s 3ms/step - loss: -15263900.0000 - MSE: 6.5129 - val_loss: -15194946.0000 - val_MSE: 6.6258\n",
            "Epoch 38/200\n",
            "138/138 [==============================] - 0s 3ms/step - loss: -16305814.0000 - MSE: 6.5129 - val_loss: -16221589.0000 - val_MSE: 6.6258\n",
            "Epoch 39/200\n",
            "138/138 [==============================] - 0s 4ms/step - loss: -17387936.0000 - MSE: 6.5129 - val_loss: -17282696.0000 - val_MSE: 6.6258\n",
            "Epoch 40/200\n",
            "138/138 [==============================] - 0s 3ms/step - loss: -18510826.0000 - MSE: 6.5129 - val_loss: -18385982.0000 - val_MSE: 6.6258\n",
            "Epoch 41/200\n",
            "138/138 [==============================] - 0s 3ms/step - loss: -19678180.0000 - MSE: 6.5129 - val_loss: -19527018.0000 - val_MSE: 6.6258\n",
            "Epoch 42/200\n",
            "138/138 [==============================] - 0s 3ms/step - loss: -20884904.0000 - MSE: 6.5129 - val_loss: -20712870.0000 - val_MSE: 6.6258\n",
            "Epoch 43/200\n",
            "138/138 [==============================] - 0s 3ms/step - loss: -22134820.0000 - MSE: 6.5129 - val_loss: -21939884.0000 - val_MSE: 6.6258\n",
            "Epoch 44/200\n",
            "138/138 [==============================] - 0s 3ms/step - loss: -23431710.0000 - MSE: 6.5129 - val_loss: -23200646.0000 - val_MSE: 6.6258\n",
            "Epoch 45/200\n",
            "138/138 [==============================] - 0s 3ms/step - loss: -24773782.0000 - MSE: 6.5129 - val_loss: -24512106.0000 - val_MSE: 6.6258\n",
            "Epoch 46/200\n",
            "138/138 [==============================] - 0s 3ms/step - loss: -26156188.0000 - MSE: 6.5129 - val_loss: -25864666.0000 - val_MSE: 6.6258\n",
            "Epoch 47/200\n",
            "138/138 [==============================] - 0s 2ms/step - loss: -27585962.0000 - MSE: 6.5129 - val_loss: -27262832.0000 - val_MSE: 6.6258\n",
            "Epoch 48/200\n",
            "138/138 [==============================] - 0s 2ms/step - loss: -29053360.0000 - MSE: 6.5129 - val_loss: -28700594.0000 - val_MSE: 6.6258\n",
            "Epoch 49/200\n",
            "138/138 [==============================] - 0s 2ms/step - loss: -30570468.0000 - MSE: 6.5129 - val_loss: -30175812.0000 - val_MSE: 6.6258\n",
            "Epoch 50/200\n",
            "138/138 [==============================] - 0s 2ms/step - loss: -32128950.0000 - MSE: 6.5129 - val_loss: -31700598.0000 - val_MSE: 6.6258\n",
            "Epoch 51/200\n",
            "138/138 [==============================] - 0s 2ms/step - loss: -33729920.0000 - MSE: 6.5129 - val_loss: -33261664.0000 - val_MSE: 6.6258\n",
            "Epoch 52/200\n",
            "138/138 [==============================] - 0s 3ms/step - loss: -35377684.0000 - MSE: 6.5129 - val_loss: -34872896.0000 - val_MSE: 6.6258\n",
            "Epoch 53/200\n",
            "138/138 [==============================] - 0s 2ms/step - loss: -37071800.0000 - MSE: 6.5129 - val_loss: -36521496.0000 - val_MSE: 6.6258\n",
            "Epoch 54/200\n",
            "138/138 [==============================] - 0s 2ms/step - loss: -38818292.0000 - MSE: 6.5129 - val_loss: -38231440.0000 - val_MSE: 6.6258\n",
            "Epoch 55/200\n",
            "138/138 [==============================] - 0s 2ms/step - loss: -40608776.0000 - MSE: 6.5129 - val_loss: -39979944.0000 - val_MSE: 6.6258\n",
            "Epoch 56/200\n",
            "138/138 [==============================] - 0s 2ms/step - loss: -42447084.0000 - MSE: 6.5129 - val_loss: -41774852.0000 - val_MSE: 6.6258\n",
            "Epoch 57/200\n",
            "138/138 [==============================] - 0s 2ms/step - loss: -44338144.0000 - MSE: 6.5129 - val_loss: -43624836.0000 - val_MSE: 6.6258\n",
            "Epoch 58/200\n",
            "138/138 [==============================] - 0s 3ms/step - loss: -46280648.0000 - MSE: 6.5129 - val_loss: -45506580.0000 - val_MSE: 6.6258\n",
            "Epoch 59/200\n",
            "138/138 [==============================] - 0s 2ms/step - loss: -48270512.0000 - MSE: 6.5129 - val_loss: -47443380.0000 - val_MSE: 6.6258\n",
            "Epoch 60/200\n",
            "138/138 [==============================] - 0s 2ms/step - loss: -50314636.0000 - MSE: 6.5129 - val_loss: -49438060.0000 - val_MSE: 6.6258\n",
            "Epoch 61/200\n",
            "138/138 [==============================] - 0s 2ms/step - loss: -52406580.0000 - MSE: 6.5129 - val_loss: -51488504.0000 - val_MSE: 6.6258\n",
            "Epoch 62/200\n",
            "138/138 [==============================] - 0s 2ms/step - loss: -54549848.0000 - MSE: 6.5129 - val_loss: -53565300.0000 - val_MSE: 6.6258\n",
            "Epoch 63/200\n",
            "138/138 [==============================] - 0s 2ms/step - loss: -56756448.0000 - MSE: 6.5129 - val_loss: -55733924.0000 - val_MSE: 6.6258\n",
            "Epoch 64/200\n",
            "138/138 [==============================] - 0s 2ms/step - loss: -58995460.0000 - MSE: 6.5129 - val_loss: -57904504.0000 - val_MSE: 6.6258\n",
            "Epoch 65/200\n",
            "138/138 [==============================] - 0s 2ms/step - loss: -61295424.0000 - MSE: 6.5129 - val_loss: -60129192.0000 - val_MSE: 6.6258\n",
            "Epoch 66/200\n",
            "138/138 [==============================] - 0s 2ms/step - loss: -63647424.0000 - MSE: 6.5129 - val_loss: -62435316.0000 - val_MSE: 6.6258\n",
            "Epoch 67/200\n",
            "138/138 [==============================] - 0s 2ms/step - loss: -66053004.0000 - MSE: 6.5129 - val_loss: -64781972.0000 - val_MSE: 6.6258\n",
            "Epoch 68/200\n",
            "138/138 [==============================] - 0s 2ms/step - loss: -68509768.0000 - MSE: 6.5129 - val_loss: -67171096.0000 - val_MSE: 6.6258\n",
            "Epoch 69/200\n",
            "138/138 [==============================] - 0s 2ms/step - loss: -71030592.0000 - MSE: 6.5129 - val_loss: -69620936.0000 - val_MSE: 6.6258\n",
            "Epoch 70/200\n",
            "138/138 [==============================] - 0s 3ms/step - loss: -73602712.0000 - MSE: 6.5129 - val_loss: -72128496.0000 - val_MSE: 6.6258\n",
            "Epoch 71/200\n",
            "138/138 [==============================] - 0s 2ms/step - loss: -76232240.0000 - MSE: 6.5129 - val_loss: -74691184.0000 - val_MSE: 6.6258\n",
            "Epoch 72/200\n",
            "138/138 [==============================] - 0s 2ms/step - loss: -78919384.0000 - MSE: 6.5129 - val_loss: -77303432.0000 - val_MSE: 6.6258\n",
            "Epoch 73/200\n",
            "138/138 [==============================] - 0s 3ms/step - loss: -81669168.0000 - MSE: 6.5129 - val_loss: -79981784.0000 - val_MSE: 6.6258\n",
            "Epoch 74/200\n",
            "138/138 [==============================] - 0s 2ms/step - loss: -84469184.0000 - MSE: 6.5129 - val_loss: -82706024.0000 - val_MSE: 6.6258\n",
            "Epoch 75/200\n",
            "138/138 [==============================] - 0s 3ms/step - loss: -87325384.0000 - MSE: 6.5129 - val_loss: -85479824.0000 - val_MSE: 6.6258\n",
            "Epoch 76/200\n",
            "138/138 [==============================] - 0s 3ms/step - loss: -90240968.0000 - MSE: 6.5129 - val_loss: -88317640.0000 - val_MSE: 6.6258\n",
            "Epoch 77/200\n",
            "138/138 [==============================] - 0s 3ms/step - loss: -93217960.0000 - MSE: 6.5129 - val_loss: -91221200.0000 - val_MSE: 6.6258\n",
            "Epoch 78/200\n",
            "138/138 [==============================] - 0s 3ms/step - loss: -96259536.0000 - MSE: 6.5129 - val_loss: -94166352.0000 - val_MSE: 6.6258\n",
            "Epoch 79/200\n",
            "138/138 [==============================] - 0s 3ms/step - loss: -99355040.0000 - MSE: 6.5129 - val_loss: -97189128.0000 - val_MSE: 6.6258\n",
            "Epoch 80/200\n",
            "138/138 [==============================] - 0s 3ms/step - loss: -102513288.0000 - MSE: 6.5129 - val_loss: -100262824.0000 - val_MSE: 6.6258\n",
            "Epoch 81/200\n",
            "138/138 [==============================] - 0s 3ms/step - loss: -105740040.0000 - MSE: 6.5129 - val_loss: -103387400.0000 - val_MSE: 6.6258\n",
            "Epoch 82/200\n",
            "138/138 [==============================] - 0s 3ms/step - loss: -109031304.0000 - MSE: 6.5129 - val_loss: -106583720.0000 - val_MSE: 6.6258\n",
            "Epoch 83/200\n",
            "138/138 [==============================] - 0s 3ms/step - loss: -112379592.0000 - MSE: 6.5129 - val_loss: -109854448.0000 - val_MSE: 6.6258\n",
            "Epoch 84/200\n",
            "138/138 [==============================] - 1s 4ms/step - loss: -115804904.0000 - MSE: 6.5129 - val_loss: -113170640.0000 - val_MSE: 6.6258\n",
            "Epoch 85/200\n",
            "138/138 [==============================] - 1s 4ms/step - loss: -119291792.0000 - MSE: 6.5129 - val_loss: -116549424.0000 - val_MSE: 6.6258\n",
            "Epoch 86/200\n",
            "138/138 [==============================] - 1s 4ms/step - loss: -122844800.0000 - MSE: 6.5129 - val_loss: -119996952.0000 - val_MSE: 6.6258\n",
            "Epoch 87/200\n",
            "138/138 [==============================] - 1s 4ms/step - loss: -126460152.0000 - MSE: 6.5129 - val_loss: -123503368.0000 - val_MSE: 6.6258\n",
            "Epoch 88/200\n",
            "138/138 [==============================] - 1s 4ms/step - loss: -130130872.0000 - MSE: 6.5129 - val_loss: -127076728.0000 - val_MSE: 6.6258\n",
            "Epoch 89/200\n",
            "138/138 [==============================] - 0s 2ms/step - loss: -133868792.0000 - MSE: 6.5129 - val_loss: -130712136.0000 - val_MSE: 6.6258\n",
            "Epoch 90/200\n",
            "138/138 [==============================] - 1s 4ms/step - loss: -137672192.0000 - MSE: 6.5129 - val_loss: -134394240.0000 - val_MSE: 6.6258\n",
            "Epoch 91/200\n",
            "138/138 [==============================] - 1s 4ms/step - loss: -141558224.0000 - MSE: 6.5129 - val_loss: -138151808.0000 - val_MSE: 6.6258\n",
            "Epoch 92/200\n",
            "138/138 [==============================] - 0s 3ms/step - loss: -145495088.0000 - MSE: 6.5129 - val_loss: -141972592.0000 - val_MSE: 6.6258\n",
            "Epoch 93/200\n",
            "138/138 [==============================] - 1s 4ms/step - loss: -149512832.0000 - MSE: 6.5129 - val_loss: -145886400.0000 - val_MSE: 6.6258\n",
            "Epoch 94/200\n",
            "138/138 [==============================] - 1s 6ms/step - loss: -153586368.0000 - MSE: 6.5129 - val_loss: -149834752.0000 - val_MSE: 6.6258\n",
            "Epoch 95/200\n",
            "138/138 [==============================] - 0s 3ms/step - loss: -157744912.0000 - MSE: 6.5129 - val_loss: -153883840.0000 - val_MSE: 6.6258\n",
            "Epoch 96/200\n",
            "138/138 [==============================] - 0s 2ms/step - loss: -161976704.0000 - MSE: 6.5129 - val_loss: -157988304.0000 - val_MSE: 6.6258\n",
            "Epoch 97/200\n",
            "138/138 [==============================] - 0s 2ms/step - loss: -166268144.0000 - MSE: 6.5129 - val_loss: -162127072.0000 - val_MSE: 6.6258\n",
            "Epoch 98/200\n",
            "138/138 [==============================] - 0s 3ms/step - loss: -170622720.0000 - MSE: 6.5129 - val_loss: -166364816.0000 - val_MSE: 6.6258\n",
            "Epoch 99/200\n",
            "138/138 [==============================] - 0s 3ms/step - loss: -175056208.0000 - MSE: 6.5129 - val_loss: -170645840.0000 - val_MSE: 6.6258\n",
            "Epoch 100/200\n",
            "138/138 [==============================] - 1s 4ms/step - loss: -179549728.0000 - MSE: 6.5129 - val_loss: -175040352.0000 - val_MSE: 6.6258\n",
            "Epoch 101/200\n",
            "138/138 [==============================] - 1s 4ms/step - loss: -184113904.0000 - MSE: 6.5129 - val_loss: -179467232.0000 - val_MSE: 6.6258\n",
            "Epoch 102/200\n",
            "138/138 [==============================] - 1s 4ms/step - loss: -188751920.0000 - MSE: 6.5129 - val_loss: -183945696.0000 - val_MSE: 6.6258\n",
            "Epoch 103/200\n",
            "138/138 [==============================] - 0s 2ms/step - loss: -193456224.0000 - MSE: 6.5129 - val_loss: -188505424.0000 - val_MSE: 6.6258\n",
            "Epoch 104/200\n",
            "138/138 [==============================] - 1s 5ms/step - loss: -198238496.0000 - MSE: 6.5129 - val_loss: -193151168.0000 - val_MSE: 6.6258\n",
            "Epoch 105/200\n",
            "138/138 [==============================] - 1s 5ms/step - loss: -203103728.0000 - MSE: 6.5129 - val_loss: -197862192.0000 - val_MSE: 6.6258\n",
            "Epoch 106/200\n",
            "138/138 [==============================] - 1s 9ms/step - loss: -208042208.0000 - MSE: 6.5129 - val_loss: -202663728.0000 - val_MSE: 6.6258\n",
            "Epoch 107/200\n",
            "138/138 [==============================] - 1s 10ms/step - loss: -213050688.0000 - MSE: 6.5129 - val_loss: -207517808.0000 - val_MSE: 6.6258\n",
            "Epoch 108/200\n",
            "138/138 [==============================] - 1s 8ms/step - loss: -218142832.0000 - MSE: 6.5129 - val_loss: -212453280.0000 - val_MSE: 6.6258\n",
            "Epoch 109/200\n",
            "138/138 [==============================] - 1s 7ms/step - loss: -223303104.0000 - MSE: 6.5129 - val_loss: -217431440.0000 - val_MSE: 6.6258\n",
            "Epoch 110/200\n",
            "138/138 [==============================] - 1s 4ms/step - loss: -228525456.0000 - MSE: 6.5129 - val_loss: -222539296.0000 - val_MSE: 6.6258\n",
            "Epoch 111/200\n",
            "138/138 [==============================] - 1s 5ms/step - loss: -233828896.0000 - MSE: 6.5129 - val_loss: -227667040.0000 - val_MSE: 6.6258\n",
            "Epoch 112/200\n",
            "138/138 [==============================] - 0s 3ms/step - loss: -239214864.0000 - MSE: 6.5129 - val_loss: -232859200.0000 - val_MSE: 6.6258\n",
            "Epoch 113/200\n",
            "138/138 [==============================] - 0s 2ms/step - loss: -244692736.0000 - MSE: 6.5129 - val_loss: -238178016.0000 - val_MSE: 6.6258\n",
            "Epoch 114/200\n",
            "138/138 [==============================] - 0s 3ms/step - loss: -250254896.0000 - MSE: 6.5129 - val_loss: -243588128.0000 - val_MSE: 6.6258\n",
            "Epoch 115/200\n",
            "138/138 [==============================] - 0s 2ms/step - loss: -255891824.0000 - MSE: 6.5129 - val_loss: -249023200.0000 - val_MSE: 6.6258\n",
            "Epoch 116/200\n",
            "138/138 [==============================] - 0s 2ms/step - loss: -261592928.0000 - MSE: 6.5129 - val_loss: -254574272.0000 - val_MSE: 6.6258\n",
            "Epoch 117/200\n",
            "138/138 [==============================] - 0s 3ms/step - loss: -267375600.0000 - MSE: 6.5129 - val_loss: -260158352.0000 - val_MSE: 6.6258\n",
            "Epoch 118/200\n",
            "138/138 [==============================] - 0s 2ms/step - loss: -273234016.0000 - MSE: 6.5129 - val_loss: -265837424.0000 - val_MSE: 6.6258\n",
            "Epoch 119/200\n",
            "138/138 [==============================] - 0s 2ms/step - loss: -279196800.0000 - MSE: 6.5129 - val_loss: -271624192.0000 - val_MSE: 6.6258\n",
            "Epoch 120/200\n",
            "138/138 [==============================] - 0s 3ms/step - loss: -285222976.0000 - MSE: 6.5129 - val_loss: -277429024.0000 - val_MSE: 6.6258\n",
            "Epoch 121/200\n",
            "138/138 [==============================] - 0s 2ms/step - loss: -291337920.0000 - MSE: 6.5129 - val_loss: -283370240.0000 - val_MSE: 6.6258\n",
            "Epoch 122/200\n",
            "138/138 [==============================] - 0s 2ms/step - loss: -297515488.0000 - MSE: 6.5129 - val_loss: -289368000.0000 - val_MSE: 6.6258\n",
            "Epoch 123/200\n",
            "138/138 [==============================] - 0s 2ms/step - loss: -303763584.0000 - MSE: 6.5129 - val_loss: -295423680.0000 - val_MSE: 6.6258\n",
            "Epoch 124/200\n",
            "138/138 [==============================] - 0s 2ms/step - loss: -310100288.0000 - MSE: 6.5129 - val_loss: -301576928.0000 - val_MSE: 6.6258\n",
            "Epoch 125/200\n",
            "138/138 [==============================] - 0s 2ms/step - loss: -316527680.0000 - MSE: 6.5129 - val_loss: -307790240.0000 - val_MSE: 6.6258\n",
            "Epoch 126/200\n",
            "138/138 [==============================] - 0s 2ms/step - loss: -323041120.0000 - MSE: 6.5129 - val_loss: -314088800.0000 - val_MSE: 6.6258\n",
            "Epoch 127/200\n",
            "138/138 [==============================] - 0s 2ms/step - loss: -329643648.0000 - MSE: 6.5129 - val_loss: -320512352.0000 - val_MSE: 6.6258\n",
            "Epoch 128/200\n",
            "138/138 [==============================] - 0s 2ms/step - loss: -336330976.0000 - MSE: 6.5129 - val_loss: -326968960.0000 - val_MSE: 6.6258\n",
            "Epoch 129/200\n",
            "138/138 [==============================] - 0s 3ms/step - loss: -343111392.0000 - MSE: 6.5129 - val_loss: -333536672.0000 - val_MSE: 6.6258\n",
            "Epoch 130/200\n",
            "138/138 [==============================] - 0s 2ms/step - loss: -349974944.0000 - MSE: 6.5129 - val_loss: -340193504.0000 - val_MSE: 6.6258\n",
            "Epoch 131/200\n",
            "138/138 [==============================] - 0s 2ms/step - loss: -356924992.0000 - MSE: 6.5129 - val_loss: -346890528.0000 - val_MSE: 6.6258\n",
            "Epoch 132/200\n",
            "138/138 [==============================] - 0s 3ms/step - loss: -363990304.0000 - MSE: 6.5129 - val_loss: -353794816.0000 - val_MSE: 6.6258\n",
            "Epoch 133/200\n",
            "138/138 [==============================] - 0s 3ms/step - loss: -371120384.0000 - MSE: 6.5129 - val_loss: -360638848.0000 - val_MSE: 6.6258\n",
            "Epoch 134/200\n",
            "138/138 [==============================] - 0s 2ms/step - loss: -378323424.0000 - MSE: 6.5129 - val_loss: -367624960.0000 - val_MSE: 6.6258\n",
            "Epoch 135/200\n",
            "138/138 [==============================] - 0s 2ms/step - loss: -385623264.0000 - MSE: 6.5129 - val_loss: -374710080.0000 - val_MSE: 6.6258\n",
            "Epoch 136/200\n",
            "138/138 [==============================] - 0s 3ms/step - loss: -392995840.0000 - MSE: 6.5129 - val_loss: -381837248.0000 - val_MSE: 6.6258\n",
            "Epoch 137/200\n",
            "138/138 [==============================] - 0s 3ms/step - loss: -400452160.0000 - MSE: 6.5129 - val_loss: -389056512.0000 - val_MSE: 6.6258\n",
            "Epoch 138/200\n",
            "138/138 [==============================] - 0s 3ms/step - loss: -408014112.0000 - MSE: 6.5129 - val_loss: -396348640.0000 - val_MSE: 6.6258\n",
            "Epoch 139/200\n",
            "138/138 [==============================] - 0s 4ms/step - loss: -415672544.0000 - MSE: 6.5129 - val_loss: -403773696.0000 - val_MSE: 6.6258\n",
            "Epoch 140/200\n",
            "138/138 [==============================] - 0s 3ms/step - loss: -423422368.0000 - MSE: 6.5129 - val_loss: -411305632.0000 - val_MSE: 6.6258\n",
            "Epoch 141/200\n",
            "138/138 [==============================] - 0s 3ms/step - loss: -431267648.0000 - MSE: 6.5129 - val_loss: -418845792.0000 - val_MSE: 6.6258\n",
            "Epoch 142/200\n",
            "138/138 [==============================] - 0s 3ms/step - loss: -439198656.0000 - MSE: 6.5129 - val_loss: -426552896.0000 - val_MSE: 6.6258\n",
            "Epoch 143/200\n",
            "138/138 [==============================] - 0s 3ms/step - loss: -447232352.0000 - MSE: 6.5129 - val_loss: -434316224.0000 - val_MSE: 6.6258\n",
            "Epoch 144/200\n",
            "138/138 [==============================] - 1s 4ms/step - loss: -455348256.0000 - MSE: 6.5129 - val_loss: -442207744.0000 - val_MSE: 6.6258\n",
            "Epoch 145/200\n",
            "138/138 [==============================] - 0s 4ms/step - loss: -463545824.0000 - MSE: 6.5129 - val_loss: -450110528.0000 - val_MSE: 6.6258\n",
            "Epoch 146/200\n",
            "138/138 [==============================] - 0s 3ms/step - loss: -471836864.0000 - MSE: 6.5129 - val_loss: -458117600.0000 - val_MSE: 6.6258\n",
            "Epoch 147/200\n",
            "138/138 [==============================] - 0s 3ms/step - loss: -480242368.0000 - MSE: 6.5129 - val_loss: -466286496.0000 - val_MSE: 6.6258\n",
            "Epoch 148/200\n",
            "138/138 [==============================] - 0s 2ms/step - loss: -488729824.0000 - MSE: 6.5129 - val_loss: -474491904.0000 - val_MSE: 6.6258\n",
            "Epoch 149/200\n",
            "138/138 [==============================] - 0s 2ms/step - loss: -497293088.0000 - MSE: 6.5129 - val_loss: -482784448.0000 - val_MSE: 6.6258\n",
            "Epoch 150/200\n",
            "138/138 [==============================] - 0s 2ms/step - loss: -505968224.0000 - MSE: 6.5129 - val_loss: -491175968.0000 - val_MSE: 6.6258\n",
            "Epoch 151/200\n",
            "138/138 [==============================] - 0s 3ms/step - loss: -514751712.0000 - MSE: 6.5129 - val_loss: -499688064.0000 - val_MSE: 6.6258\n",
            "Epoch 152/200\n",
            "138/138 [==============================] - 0s 2ms/step - loss: -523619040.0000 - MSE: 6.5129 - val_loss: -508263040.0000 - val_MSE: 6.6258\n",
            "Epoch 153/200\n",
            "138/138 [==============================] - 0s 2ms/step - loss: -532583968.0000 - MSE: 6.5129 - val_loss: -516938816.0000 - val_MSE: 6.6258\n",
            "Epoch 154/200\n",
            "138/138 [==============================] - 0s 3ms/step - loss: -541661376.0000 - MSE: 6.5129 - val_loss: -525736160.0000 - val_MSE: 6.6258\n",
            "Epoch 155/200\n",
            "138/138 [==============================] - 0s 3ms/step - loss: -550822656.0000 - MSE: 6.5129 - val_loss: -534603872.0000 - val_MSE: 6.6258\n",
            "Epoch 156/200\n",
            "138/138 [==============================] - 0s 2ms/step - loss: -560083328.0000 - MSE: 6.5129 - val_loss: -543558400.0000 - val_MSE: 6.6258\n",
            "Epoch 157/200\n",
            "138/138 [==============================] - 0s 2ms/step - loss: -569438272.0000 - MSE: 6.5129 - val_loss: -552600960.0000 - val_MSE: 6.6258\n",
            "Epoch 158/200\n",
            "138/138 [==============================] - 0s 2ms/step - loss: -578890688.0000 - MSE: 6.5129 - val_loss: -561778560.0000 - val_MSE: 6.6258\n",
            "Epoch 159/200\n",
            "138/138 [==============================] - 0s 3ms/step - loss: -588441344.0000 - MSE: 6.5129 - val_loss: -571010624.0000 - val_MSE: 6.6258\n",
            "Epoch 160/200\n",
            "138/138 [==============================] - 0s 2ms/step - loss: -598108992.0000 - MSE: 6.5129 - val_loss: -580326656.0000 - val_MSE: 6.6258\n",
            "Epoch 161/200\n",
            "138/138 [==============================] - 0s 2ms/step - loss: -607882560.0000 - MSE: 6.5129 - val_loss: -589794176.0000 - val_MSE: 6.6258\n",
            "Epoch 162/200\n",
            "138/138 [==============================] - 0s 3ms/step - loss: -617745792.0000 - MSE: 6.5129 - val_loss: -599335936.0000 - val_MSE: 6.6258\n",
            "Epoch 163/200\n",
            "138/138 [==============================] - 0s 2ms/step - loss: -627719744.0000 - MSE: 6.5129 - val_loss: -608967104.0000 - val_MSE: 6.6258\n",
            "Epoch 164/200\n",
            "138/138 [==============================] - 0s 2ms/step - loss: -637841664.0000 - MSE: 6.5129 - val_loss: -618723840.0000 - val_MSE: 6.6258\n",
            "Epoch 165/200\n",
            "138/138 [==============================] - 0s 2ms/step - loss: -648000320.0000 - MSE: 6.5129 - val_loss: -628538304.0000 - val_MSE: 6.6258\n",
            "Epoch 166/200\n",
            "138/138 [==============================] - 0s 3ms/step - loss: -658286336.0000 - MSE: 6.5129 - val_loss: -638534016.0000 - val_MSE: 6.6258\n",
            "Epoch 167/200\n",
            "138/138 [==============================] - 0s 3ms/step - loss: -668692288.0000 - MSE: 6.5129 - val_loss: -648579520.0000 - val_MSE: 6.6258\n",
            "Epoch 168/200\n",
            "138/138 [==============================] - 0s 3ms/step - loss: -679185216.0000 - MSE: 6.5129 - val_loss: -658728896.0000 - val_MSE: 6.6258\n",
            "Epoch 169/200\n",
            "138/138 [==============================] - 0s 2ms/step - loss: -689799808.0000 - MSE: 6.5129 - val_loss: -669030464.0000 - val_MSE: 6.6258\n",
            "Epoch 170/200\n",
            "138/138 [==============================] - 0s 3ms/step - loss: -700489856.0000 - MSE: 6.5129 - val_loss: -679373824.0000 - val_MSE: 6.6258\n",
            "Epoch 171/200\n",
            "138/138 [==============================] - 0s 3ms/step - loss: -711280576.0000 - MSE: 6.5129 - val_loss: -689758336.0000 - val_MSE: 6.6258\n",
            "Epoch 172/200\n",
            "138/138 [==============================] - 0s 2ms/step - loss: -722229632.0000 - MSE: 6.5129 - val_loss: -700343360.0000 - val_MSE: 6.6258\n",
            "Epoch 173/200\n",
            "138/138 [==============================] - 0s 3ms/step - loss: -733216192.0000 - MSE: 6.5129 - val_loss: -711009536.0000 - val_MSE: 6.6258\n",
            "Epoch 174/200\n",
            "138/138 [==============================] - 0s 3ms/step - loss: -744319680.0000 - MSE: 6.5129 - val_loss: -721712640.0000 - val_MSE: 6.6258\n",
            "Epoch 175/200\n",
            "138/138 [==============================] - 0s 3ms/step - loss: -755565760.0000 - MSE: 6.5129 - val_loss: -732608320.0000 - val_MSE: 6.6258\n",
            "Epoch 176/200\n",
            "138/138 [==============================] - 0s 3ms/step - loss: -766884800.0000 - MSE: 6.5129 - val_loss: -743552896.0000 - val_MSE: 6.6258\n",
            "Epoch 177/200\n",
            "138/138 [==============================] - 0s 3ms/step - loss: -778306368.0000 - MSE: 6.5129 - val_loss: -754574208.0000 - val_MSE: 6.6258\n",
            "Epoch 178/200\n",
            "138/138 [==============================] - 0s 3ms/step - loss: -789861120.0000 - MSE: 6.5129 - val_loss: -765696768.0000 - val_MSE: 6.6258\n",
            "Epoch 179/200\n",
            "138/138 [==============================] - 0s 3ms/step - loss: -801506368.0000 - MSE: 6.5129 - val_loss: -777068416.0000 - val_MSE: 6.6258\n",
            "Epoch 180/200\n",
            "138/138 [==============================] - 0s 3ms/step - loss: -813259968.0000 - MSE: 6.5129 - val_loss: -788391296.0000 - val_MSE: 6.6258\n",
            "Epoch 181/200\n",
            "138/138 [==============================] - 0s 3ms/step - loss: -825135104.0000 - MSE: 6.5129 - val_loss: -799895488.0000 - val_MSE: 6.6258\n",
            "Epoch 182/200\n",
            "138/138 [==============================] - 0s 3ms/step - loss: -837130112.0000 - MSE: 6.5129 - val_loss: -811463616.0000 - val_MSE: 6.6258\n",
            "Epoch 183/200\n",
            "138/138 [==============================] - 0s 3ms/step - loss: -849261632.0000 - MSE: 6.5129 - val_loss: -823169408.0000 - val_MSE: 6.6258\n",
            "Epoch 184/200\n",
            "138/138 [==============================] - 0s 3ms/step - loss: -861480384.0000 - MSE: 6.5129 - val_loss: -835013568.0000 - val_MSE: 6.6258\n",
            "Epoch 185/200\n",
            "138/138 [==============================] - 0s 3ms/step - loss: -873809344.0000 - MSE: 6.5129 - val_loss: -846945216.0000 - val_MSE: 6.6258\n",
            "Epoch 186/200\n",
            "138/138 [==============================] - 0s 3ms/step - loss: -886226112.0000 - MSE: 6.5129 - val_loss: -858963008.0000 - val_MSE: 6.6258\n",
            "Epoch 187/200\n",
            "138/138 [==============================] - 0s 3ms/step - loss: -898776064.0000 - MSE: 6.5129 - val_loss: -871138624.0000 - val_MSE: 6.6258\n",
            "Epoch 188/200\n",
            "138/138 [==============================] - 0s 3ms/step - loss: -911472704.0000 - MSE: 6.5129 - val_loss: -883370816.0000 - val_MSE: 6.6258\n",
            "Epoch 189/200\n",
            "138/138 [==============================] - 0s 2ms/step - loss: -924270400.0000 - MSE: 6.5129 - val_loss: -895722304.0000 - val_MSE: 6.6258\n",
            "Epoch 190/200\n",
            "138/138 [==============================] - 0s 2ms/step - loss: -937191872.0000 - MSE: 6.5129 - val_loss: -908220608.0000 - val_MSE: 6.6258\n",
            "Epoch 191/200\n",
            "138/138 [==============================] - 0s 2ms/step - loss: -950241088.0000 - MSE: 6.5129 - val_loss: -920814912.0000 - val_MSE: 6.6258\n",
            "Epoch 192/200\n",
            "138/138 [==============================] - 0s 2ms/step - loss: -963441024.0000 - MSE: 6.5129 - val_loss: -933583808.0000 - val_MSE: 6.6258\n",
            "Epoch 193/200\n",
            "138/138 [==============================] - 0s 2ms/step - loss: -976761728.0000 - MSE: 6.5129 - val_loss: -946428032.0000 - val_MSE: 6.6258\n",
            "Epoch 194/200\n",
            "138/138 [==============================] - 0s 2ms/step - loss: -990162560.0000 - MSE: 6.5129 - val_loss: -959405248.0000 - val_MSE: 6.6258\n",
            "Epoch 195/200\n",
            "138/138 [==============================] - 0s 2ms/step - loss: -1003680064.0000 - MSE: 6.5129 - val_loss: -972443200.0000 - val_MSE: 6.6258\n",
            "Epoch 196/200\n",
            "138/138 [==============================] - 0s 2ms/step - loss: -1017320512.0000 - MSE: 6.5129 - val_loss: -985606784.0000 - val_MSE: 6.6258\n",
            "Epoch 197/200\n",
            "138/138 [==============================] - 0s 2ms/step - loss: -1031067968.0000 - MSE: 6.5129 - val_loss: -998872896.0000 - val_MSE: 6.6258\n",
            "Epoch 198/200\n",
            "138/138 [==============================] - 0s 2ms/step - loss: -1044941760.0000 - MSE: 6.5129 - val_loss: -1012320384.0000 - val_MSE: 6.6258\n",
            "Epoch 199/200\n",
            "138/138 [==============================] - 0s 3ms/step - loss: -1058924096.0000 - MSE: 6.5129 - val_loss: -1025827840.0000 - val_MSE: 6.6258\n",
            "Epoch 200/200\n",
            "138/138 [==============================] - 0s 3ms/step - loss: -1073033216.0000 - MSE: 6.5129 - val_loss: -1039453696.0000 - val_MSE: 6.6258\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get Paramter Values for GMM function\n",
        "import tensorflow as tf\n",
        "\n",
        "params = model.get_weights()\n",
        "\n",
        "\"\"\"\n",
        "print(params[4][23][0]) #vs_30\n",
        "print(params[3][30]) # dist_rup\n",
        "print(params[2][1][31]) #mag\n",
        "\"\"\"\n",
        "\n",
        "vs30 = params[4][23][0]\n",
        "dist_rup = params[3][30]\n",
        "mag = params[2][1][31]"
      ],
      "metadata": {
        "id": "C4IoKuXbyK_7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Idriss (2014, :cite:`idriss14`) model.\"\"\"\n",
        "import numpy as np\n",
        "\n",
        "from pygmm import model\n",
        "\n",
        "__author__ = \"Albert Kottke\"\n",
        "\n",
        "\n",
        "class Idriss2014(model.GroundMotionModel):\n",
        "    \"\"\"Idriss (2014, :cite:`idriss14`) model.\n",
        "\n",
        "    This model was developed for active tectonic regions as part of the\n",
        "    NGA-West2 effort.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    scenario : :class:`pygmm.model.Scenario`\n",
        "        earthquake scenario\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    NAME = \"Idriss (2014)\"\n",
        "    ABBREV = \"I14\"\n",
        "\n",
        "    # Reference velocity (m/s)\n",
        "    V_REF = 1200.0\n",
        "\n",
        "    # Load the coefficients for the model\n",
        "    COEFF = dict(\n",
        "        small=model.load_data_file(\"idriss_2014-small.csv\", 2),\n",
        "        large=model.load_data_file(\"idriss_2014-large.csv\", 2),\n",
        "    )\n",
        "    PERIODS = COEFF[\"small\"][\"period\"]\n",
        "\n",
        "    INDEX_PGA = 0\n",
        "    INDICES_PSA = np.arange(22)\n",
        "\n",
        "    PARAMS = [\n",
        "        model.NumericParameter(\"dist_rup\", True, None, 150),\n",
        "        model.NumericParameter(\"mag\", True, 5, None),\n",
        "        model.NumericParameter(\"v_s30\", True, 450, 1200),\n",
        "        model.CategoricalParameter(\"mechanism\", True, [\"SS\", \"RS\"], \"SS\"),\n",
        "    ]\n",
        "\n",
        "\n",
        "    def __init__(self, scenario: model.Scenario):\n",
        "        \"\"\"Initialize the model.\"\"\"\n",
        "        super().__init__(scenario)\n",
        "        self._ln_resp = self._calc_ln_resp()\n",
        "        self._ln_std = self._calc_ln_std()\n",
        "\n",
        "\n",
        "\n",
        "    def _calc_ln_resp(self) -> np.ndarray:\n",
        "        \"\"\"Calculate the natural logarithm of the response.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        ln_resp : class:`np.array`:\n",
        "            natural log of the response\n",
        "\n",
        "        \"\"\"\n",
        "        s = self._scenario\n",
        "        c = self.COEFF[\"small\"] if s.mag <= 6.75 else self.COEFF[\"large\"]\n",
        "\n",
        "        if s.mechanism == \"RS\":\n",
        "            flag_mech = 1\n",
        "        else:\n",
        "            # SS/RS/U\n",
        "            flag_mech = 0\n",
        "\n",
        "        f_mag = c.alpha_1 + c.alpha_2 * s.mag + c.alpha_3 * (8.5 - s.mag) ** 2\n",
        "        f_dst = (\n",
        "            -(c.beta_1 + c.beta_2 * s.mag) * np.log(s.dist_rup + 10)\n",
        "            + c.gamma * s.dist_rup\n",
        "        )\n",
        "        f_ste = c.epsilon * np.log(s.v_s30)\n",
        "        f_mec = c.phi * flag_mech\n",
        "\n",
        "        ln_resp = f_mag + f_dst + f_ste + f_mec\n",
        "\n",
        "        return ln_resp\n",
        "\n",
        "    def _calc_ln_std(self) -> np.ndarray:\n",
        "        \"\"\"Calculate the logarithmic standard deviation.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        ln_std : class:`np.array`:\n",
        "            natural log standard deviation\n",
        "\n",
        "        \"\"\"\n",
        "        s = self._scenario\n",
        "        ln_std = (\n",
        "            1.18\n",
        "            + 0.035 * np.log(np.clip(self.PERIODS, 0.05, 3.0))\n",
        "            - 0.06 * np.clip(s.mag, 5.0, 7.5)\n",
        "        )\n",
        "        return ln_std"
      ],
      "metadata": {
        "id": "oxfobxXd0Oi_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pygmm.model import Scenario"
      ],
      "metadata": {
        "id": "NlO6A9as16MT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an instance of the predictors\n",
        "scenario = Scenario(\n",
        "    mag= mag,\n",
        "    dist_rup= dist_rup,\n",
        "    v_s30=vs30,\n",
        "    mechanism=\"SS\"\n",
        ")"
      ],
      "metadata": {
        "id": "kufxJ16F169n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Idriss2014(scenario)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aQYVCEFR19lV",
        "outputId": "e074b93e-6df5-403f-ba0b-673a41209ac4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:v_s30 (-0.355417) is less than the recommended limit (450).\n",
            "<ipython-input-99-594425a62087>:77: RuntimeWarning: invalid value encountered in log\n",
            "  f_ste = c.epsilon * np.log(s.v_s30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate Metrics\n",
        "ln_resp = model._calc_ln_resp()\n",
        "ln_std = model._calc_ln_std()\n",
        "# Print Accuracy\n",
        "print(np.mean(ln_std))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TUL5GJbq2A9a",
        "outputId": "776317c7-db90-4cf6-ddd9-0a601d8e85f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.6961553421379612\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-99-594425a62087>:77: RuntimeWarning: invalid value encountered in log\n",
            "  f_ste = c.epsilon * np.log(s.v_s30)\n"
          ]
        }
      ]
    }
  ]
}